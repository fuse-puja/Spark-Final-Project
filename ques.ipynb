{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries and initialize a SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum\n",
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/12 07:57:53 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "# Initialize a Spark session\n",
    "spark = SparkSession.builder.appName(\"TopProductCategoriesRevenue\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the necessary datasets\n",
    "products_df = spark.read.csv(\"Data/olist_products_dataset.csv\", header=True, inferSchema=True)\n",
    "order_items_df = spark.read.csv(\"Data/olist_order_items_dataset.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the necessary dataset\n",
    "payments_df = spark.read.csv(\"Data/olist_order_payments_dataset.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `Order_delivered_customer_date` cannot be resolved. Did you mean one of the following? [`seller_state`, `order_item_id`, `order_item_id`, `shipping_limit_date`, `shipping_limit_date`].;\n'Project [order_id#2574, order_item_id#2575, product_id#2576, seller_id#2577, shipping_limit_date#2578, price#2579, freight_value#2580, seller_id#2549, seller_zip_code_prefix#2550, seller_city#2551, seller_state#2552, order_id#2605, order_item_id#2606, product_id#2607, seller_id#2608, shipping_limit_date#2609, price#2610, freight_value#2611, ((cast(cast('Order_delivered_customer_date as timestamp) as bigint) - cast(cast('Order_delivered_carrier_data as timestamp) as bigint)) / 86400) AS Delivery_Time#2711]\n+- Join Inner, (Order_id#2574 = Order_id#2605)\n   :- Join Inner, (Seller_id#2577 = Seller_id#2549)\n   :  :- Relation [order_id#2574,order_item_id#2575,product_id#2576,seller_id#2577,shipping_limit_date#2578,price#2579,freight_value#2580] csv\n   :  +- Relation [seller_id#2549,seller_zip_code_prefix#2550,seller_city#2551,seller_state#2552] csv\n   +- Relation [order_id#2605,order_item_id#2606,product_id#2607,seller_id#2608,shipping_limit_date#2609,price#2610,freight_value#2611] csv\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/user/Documents/Fusemachines/Spark_FInal_Projetc/ques.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/user/Documents/Fusemachines/Spark_FInal_Projetc/ques.ipynb#X24sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m reviews_df \u001b[39m=\u001b[39m spark\u001b[39m.\u001b[39mread\u001b[39m.\u001b[39mcsv(\u001b[39m\"\u001b[39m\u001b[39mData/olist_order_reviews_dataset.csv\u001b[39m\u001b[39m\"\u001b[39m, header\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, inferSchema\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/Documents/Fusemachines/Spark_FInal_Projetc/ques.ipynb#X24sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m# Calculate the average delivery time for each seller\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/Documents/Fusemachines/Spark_FInal_Projetc/ques.ipynb#X24sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m delivery_time_df \u001b[39m=\u001b[39m (\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/Documents/Fusemachines/Spark_FInal_Projetc/ques.ipynb#X24sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     order_items_df\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/Documents/Fusemachines/Spark_FInal_Projetc/ques.ipynb#X24sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39m.\u001b[39;49mjoin(seller_df, order_items_df[\u001b[39m\"\u001b[39;49m\u001b[39mSeller_id\u001b[39;49m\u001b[39m\"\u001b[39;49m] \u001b[39m==\u001b[39;49m seller_df[\u001b[39m\"\u001b[39;49m\u001b[39mSeller_id\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/Documents/Fusemachines/Spark_FInal_Projetc/ques.ipynb#X24sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     \u001b[39m.\u001b[39;49mjoin(orders_df, order_items_df[\u001b[39m\"\u001b[39;49m\u001b[39mOrder_id\u001b[39;49m\u001b[39m\"\u001b[39;49m] \u001b[39m==\u001b[39;49m orders_df[\u001b[39m\"\u001b[39;49m\u001b[39mOrder_id\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/user/Documents/Fusemachines/Spark_FInal_Projetc/ques.ipynb#X24sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     \u001b[39m.\u001b[39;49mwithColumn(\u001b[39m\"\u001b[39;49m\u001b[39mDelivery_Time\u001b[39;49m\u001b[39m\"\u001b[39;49m, (col(\u001b[39m\"\u001b[39;49m\u001b[39mOrder_delivered_customer_date\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39;49mcast(\u001b[39m\"\u001b[39;49m\u001b[39mtimestamp\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39;49mcast(\u001b[39m\"\u001b[39;49m\u001b[39mlong\u001b[39;49m\u001b[39m\"\u001b[39;49m) \u001b[39m-\u001b[39;49m col(\u001b[39m\"\u001b[39;49m\u001b[39mOrder_delivered_carrier_data\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39;49mcast(\u001b[39m\"\u001b[39;49m\u001b[39mtimestamp\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39;49mcast(\u001b[39m\"\u001b[39;49m\u001b[39mlong\u001b[39;49m\u001b[39m\"\u001b[39;49m)) \u001b[39m/\u001b[39;49m (\u001b[39m3600\u001b[39;49m \u001b[39m*\u001b[39;49m \u001b[39m24\u001b[39;49m))  \u001b[39m# Calculate delivery time in days\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/Documents/Fusemachines/Spark_FInal_Projetc/ques.ipynb#X24sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     \u001b[39m.\u001b[39mgroupBy(seller_df[\u001b[39m\"\u001b[39m\u001b[39mSeller_id\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/Documents/Fusemachines/Spark_FInal_Projetc/ques.ipynb#X24sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     \u001b[39m.\u001b[39magg(avg(\u001b[39m\"\u001b[39m\u001b[39mDelivery_Time\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39malias(\u001b[39m\"\u001b[39m\u001b[39mAvg_Delivery_Time\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/Documents/Fusemachines/Spark_FInal_Projetc/ques.ipynb#X24sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     \u001b[39m.\u001b[39morderBy(\u001b[39m\"\u001b[39m\u001b[39mAvg_Delivery_Time\u001b[39m\u001b[39m\"\u001b[39m, ascending\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/Documents/Fusemachines/Spark_FInal_Projetc/ques.ipynb#X24sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/Documents/Fusemachines/Spark_FInal_Projetc/ques.ipynb#X24sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39m# Identify sellers with the highest and lowest customer ratings\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/Documents/Fusemachines/Spark_FInal_Projetc/ques.ipynb#X24sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m seller_ratings_df \u001b[39m=\u001b[39m (\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/Documents/Fusemachines/Spark_FInal_Projetc/ques.ipynb#X24sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     seller_df\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/Documents/Fusemachines/Spark_FInal_Projetc/ques.ipynb#X24sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     \u001b[39m.\u001b[39mjoin(order_items_df, seller_df[\u001b[39m\"\u001b[39m\u001b[39mSeller_id\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m==\u001b[39m order_items_df[\u001b[39m\"\u001b[39m\u001b[39mSeller_id\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/Documents/Fusemachines/Spark_FInal_Projetc/ques.ipynb#X24sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     \u001b[39m.\u001b[39morderBy(\u001b[39m\"\u001b[39m\u001b[39mAvg_Review_Score\u001b[39m\u001b[39m\"\u001b[39m, ascending\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/Documents/Fusemachines/Spark_FInal_Projetc/ques.ipynb#X24sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/dataframe.py:4789\u001b[0m, in \u001b[0;36mDataFrame.withColumn\u001b[0;34m(self, colName, col)\u001b[0m\n\u001b[1;32m   4784\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(col, Column):\n\u001b[1;32m   4785\u001b[0m     \u001b[39mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m   4786\u001b[0m         error_class\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNOT_COLUMN\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   4787\u001b[0m         message_parameters\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39marg_name\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mcol\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39marg_type\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mtype\u001b[39m(col)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m},\n\u001b[1;32m   4788\u001b[0m     )\n\u001b[0;32m-> 4789\u001b[0m \u001b[39mreturn\u001b[39;00m DataFrame(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jdf\u001b[39m.\u001b[39;49mwithColumn(colName, col\u001b[39m.\u001b[39;49m_jc), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msparkSession)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:175\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    171\u001b[0m converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n\u001b[1;32m    172\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    173\u001b[0m     \u001b[39m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     \u001b[39m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m     \u001b[39mraise\u001b[39;00m converted \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `Order_delivered_customer_date` cannot be resolved. Did you mean one of the following? [`seller_state`, `order_item_id`, `order_item_id`, `shipping_limit_date`, `shipping_limit_date`].;\n'Project [order_id#2574, order_item_id#2575, product_id#2576, seller_id#2577, shipping_limit_date#2578, price#2579, freight_value#2580, seller_id#2549, seller_zip_code_prefix#2550, seller_city#2551, seller_state#2552, order_id#2605, order_item_id#2606, product_id#2607, seller_id#2608, shipping_limit_date#2609, price#2610, freight_value#2611, ((cast(cast('Order_delivered_customer_date as timestamp) as bigint) - cast(cast('Order_delivered_carrier_data as timestamp) as bigint)) / 86400) AS Delivery_Time#2711]\n+- Join Inner, (Order_id#2574 = Order_id#2605)\n   :- Join Inner, (Seller_id#2577 = Seller_id#2549)\n   :  :- Relation [order_id#2574,order_item_id#2575,product_id#2576,seller_id#2577,shipping_limit_date#2578,price#2579,freight_value#2580] csv\n   :  +- Relation [seller_id#2549,seller_zip_code_prefix#2550,seller_city#2551,seller_state#2552] csv\n   +- Relation [order_id#2605,order_item_id#2606,product_id#2607,seller_id#2608,shipping_limit_date#2609,price#2610,freight_value#2611] csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql.functions import avg, col\n",
    "\n",
    "\n",
    "\n",
    "# Load the required datasets into DataFrames\n",
    "seller_df = spark.read.csv(\"Data/olist_sellers_dataset.csv\", header=True, inferSchema=True)\n",
    "order_items_df = spark.read.csv(\"Data/olist_order_items_dataset.csv\", header=True, inferSchema=True)\n",
    "orders_df = spark.read.csv(\"Data/olist_order_items_dataset.csv\", header=True, inferSchema=True)\n",
    "reviews_df = spark.read.csv(\"Data/olist_order_reviews_dataset.csv\", header=True, inferSchema=True)\n",
    "# Calculate the average delivery time for each seller\n",
    "delivery_time_df = (\n",
    "    order_items_df\n",
    "    .join(seller_df, order_items_df[\"Seller_id\"] == seller_df[\"Seller_id\"])\n",
    "    .join(orders_df, order_items_df[\"Order_id\"] == orders_df[\"Order_id\"])\n",
    "    .withColumn(\"Delivery_Time\", (col(\"Order_delivered_customer_date\").cast(\"timestamp\").cast(\"long\") - col(\"Order_delivered_carrier_data\").cast(\"timestamp\").cast(\"long\")) / (3600 * 24))  # Calculate delivery time in days\n",
    "    .groupBy(seller_df[\"Seller_id\"])\n",
    "    .agg(avg(\"Delivery_Time\").alias(\"Avg_Delivery_Time\"))\n",
    "    .orderBy(\"Avg_Delivery_Time\", ascending=False)\n",
    ")\n",
    "\n",
    "# Identify sellers with the highest and lowest customer ratings\n",
    "seller_ratings_df = (\n",
    "    seller_df\n",
    "    .join(order_items_df, seller_df[\"Seller_id\"] == order_items_df[\"Seller_id\"])\n",
    "    .join(orders_df, order_items_df[\"Order_id\"] == orders_df[\"Order_id\"])\n",
    "    .join(reviews_df, orders_df[\"Order_id\"] == reviews_df[\"order_id\"], \"left\")\n",
    "    .groupBy(seller_df[\"Seller_id\"])\n",
    "    .agg(avg(col(\"review_score\").cast(\"float\")).alias(\"Avg_Review_Score\"))\n",
    "    .orderBy(\"Avg_Review_Score\", ascending=False)\n",
    ")\n",
    "\n",
    "# Show the results\n",
    "delivery_time_df.show()\n",
    "seller_ratings_df.show()\n",
    "# # Calculate the average delivery time for each seller\n",
    "# delivery_time_df = (\n",
    "#     order_items_df\n",
    "#     .join(seller_df, order_items_df.Seller_id == seller_df.Seller_id)\n",
    "#     .join(orders_df, order_items_df.Order_id == orders_df.Order_id)\n",
    "#     .withColumn(\"Delivery_Time\", (col(\"Order_delivered_customer_date\").cast(\"timestamp\").cast(\"long\") - col(\"Order_delivered_carrier_data\").cast(\"timestamp\").cast(\"long\")) / (3600 * 24))  # Calculate delivery time in days\n",
    "#     .groupBy(\"seller_df.Seller_id\")\n",
    "#     .agg(avg(\"Delivery_Time\").alias(\"Avg_Delivery_Time\"))\n",
    "#     .orderBy(\"Avg_Delivery_Time\", ascending=False)\n",
    "# )\n",
    "\n",
    "# # Identify sellers with the highest and lowest customer ratings\n",
    "# seller_ratings_df = (\n",
    "#     seller_df\n",
    "#     .join(order_items_df, seller_df.Seller_id == order_items_df.Seller_id)\n",
    "#     .join(orders_df, order_items_df.Order_id == orders_df.Order_id)\n",
    "#     .join(reviews_df, orders_df.Order_id == reviews_df.order_id, \"left\")\n",
    "#     .groupBy(\"seller_df.Seller_id\")\n",
    "#     .agg(avg(col(\"review_score\").cast(\"float\")).alias(\"Avg_Review_Score\"))\n",
    "#     .orderBy(\"Avg_Review_Score\", ascending=False)\n",
    "# )\n",
    "\n",
    "# # Show the results\n",
    "# delivery_time_df.show()\n",
    "# seller_ratings_df.show()\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[DATATYPE_MISMATCH.FILTER_NOT_BOOLEAN] Cannot resolve \"Payment_type\" due to data type mismatch: Filter expression \"Payment_type\" of type \"STRING\" is not a boolean.;\nFilter Payment_type#1357: string\n+- SubqueryAlias payments\n   +- SubqueryAlias payments\n      +- SubqueryAlias payments\n         +- SubqueryAlias payments\n            +- SubqueryAlias payments\n               +- SubqueryAlias payments\n                  +- SubqueryAlias payments\n                     +- SubqueryAlias payments\n                        +- Relation [order_id#1355,payment_sequential#1356,payment_type#1357,payment_installments#1358,payment_value#1359] csv\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/user/Documents/Fusemachines/Spark_FInal_Projetc/ques.ipynb Cell 5\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/Documents/Fusemachines/Spark_FInal_Projetc/ques.ipynb#X22sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m potential_fraud_orders \u001b[39m=\u001b[39m payments_df\u001b[39m.\u001b[39mjoin(payment_count_per_order, \u001b[39m\"\u001b[39m\u001b[39mOrder_id\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39minner\u001b[39m\u001b[39m\"\u001b[39m) \\\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/Documents/Fusemachines/Spark_FInal_Projetc/ques.ipynb#X22sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m                                     \u001b[39m.\u001b[39mjoin(payment_stats_per_order, \u001b[39m\"\u001b[39m\u001b[39mOrder_id\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39minner\u001b[39m\u001b[39m\"\u001b[39m) \\\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/Documents/Fusemachines/Spark_FInal_Projetc/ques.ipynb#X22sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m                                     \u001b[39m.\u001b[39mfilter((col(\u001b[39m\"\u001b[39m\u001b[39mpayment_method_count\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m>\u001b[39m max_payment_methods) \u001b[39m|\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/Documents/Fusemachines/Spark_FInal_Projetc/ques.ipynb#X22sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m                                             (col(\u001b[39m\"\u001b[39m\u001b[39mavg_installments\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m>\u001b[39m max_avg_installments) \u001b[39m|\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/Documents/Fusemachines/Spark_FInal_Projetc/ques.ipynb#X22sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m                                             (col(\u001b[39m\"\u001b[39m\u001b[39mstddev_installments\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m>\u001b[39m max_stddev_installments)) \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/Documents/Fusemachines/Spark_FInal_Projetc/ques.ipynb#X22sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39m# potential_fraud_orders.select(\"payments.Order_id\", \"payments.Payment_type\".alias(\"payment_type\"),\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/Documents/Fusemachines/Spark_FInal_Projetc/ques.ipynb#X22sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39m#                                             \"payments.Payment_installments\",\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/Documents/Fusemachines/Spark_FInal_Projetc/ques.ipynb#X22sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39m#                                             col(\"payment_method_count\"), col(\"avg_installments\"), col(\"stddev_installments\"))\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/Documents/Fusemachines/Spark_FInal_Projetc/ques.ipynb#X22sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m selected_columns \u001b[39m=\u001b[39m potential_fraud_orders\u001b[39m.\u001b[39mselect(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/Documents/Fusemachines/Spark_FInal_Projetc/ques.ipynb#X22sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m     payments_df[\u001b[39m\"\u001b[39m\u001b[39mOrder_id\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/user/Documents/Fusemachines/Spark_FInal_Projetc/ques.ipynb#X22sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m     payments_df[col(\u001b[39m\"\u001b[39;49m\u001b[39mPayment_type\u001b[39;49m\u001b[39m\"\u001b[39;49m)],\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/Documents/Fusemachines/Spark_FInal_Projetc/ques.ipynb#X22sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m     payments_df[col(\u001b[39m\"\u001b[39m\u001b[39mPayment_installments\u001b[39m\u001b[39m\"\u001b[39m)],\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/Documents/Fusemachines/Spark_FInal_Projetc/ques.ipynb#X22sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m     col(\u001b[39m\"\u001b[39m\u001b[39mpayment_method_count\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/Documents/Fusemachines/Spark_FInal_Projetc/ques.ipynb#X22sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m     col(\u001b[39m\"\u001b[39m\u001b[39mavg_installments\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/Documents/Fusemachines/Spark_FInal_Projetc/ques.ipynb#X22sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m     col(\u001b[39m\"\u001b[39m\u001b[39mstddev_installments\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/Documents/Fusemachines/Spark_FInal_Projetc/ques.ipynb#X22sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/Documents/Fusemachines/Spark_FInal_Projetc/ques.ipynb#X22sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m \u001b[39m# Show potential fraud orders\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/Documents/Fusemachines/Spark_FInal_Projetc/ques.ipynb#X22sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m selected_columns\u001b[39m.\u001b[39mshow()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/dataframe.py:2931\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   2929\u001b[0m     \u001b[39mreturn\u001b[39;00m Column(jc)\n\u001b[1;32m   2930\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(item, Column):\n\u001b[0;32m-> 2931\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfilter(item)\n\u001b[1;32m   2932\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(item, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)):\n\u001b[1;32m   2933\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mselect(\u001b[39m*\u001b[39mitem)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/dataframe.py:3138\u001b[0m, in \u001b[0;36mDataFrame.filter\u001b[0;34m(self, condition)\u001b[0m\n\u001b[1;32m   3136\u001b[0m     jdf \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jdf\u001b[39m.\u001b[39mfilter(condition)\n\u001b[1;32m   3137\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(condition, Column):\n\u001b[0;32m-> 3138\u001b[0m     jdf \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jdf\u001b[39m.\u001b[39;49mfilter(condition\u001b[39m.\u001b[39;49m_jc)\n\u001b[1;32m   3139\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   3140\u001b[0m     \u001b[39mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m   3141\u001b[0m         error_class\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNOT_COLUMN_OR_STR\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   3142\u001b[0m         message_parameters\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39marg_name\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mcondition\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39marg_type\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mtype\u001b[39m(condition)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m},\n\u001b[1;32m   3143\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:175\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    171\u001b[0m converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n\u001b[1;32m    172\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    173\u001b[0m     \u001b[39m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     \u001b[39m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m     \u001b[39mraise\u001b[39;00m converted \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [DATATYPE_MISMATCH.FILTER_NOT_BOOLEAN] Cannot resolve \"Payment_type\" due to data type mismatch: Filter expression \"Payment_type\" of type \"STRING\" is not a boolean.;\nFilter Payment_type#1357: string\n+- SubqueryAlias payments\n   +- SubqueryAlias payments\n      +- SubqueryAlias payments\n         +- SubqueryAlias payments\n            +- SubqueryAlias payments\n               +- SubqueryAlias payments\n                  +- SubqueryAlias payments\n                     +- SubqueryAlias payments\n                        +- Relation [order_id#1355,payment_sequential#1356,payment_type#1357,payment_installments#1358,payment_value#1359] csv\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, count, avg, stddev\n",
    "window_spec = Window.partitionBy(\"Order_id\")\n",
    "\n",
    "\n",
    "\n",
    "# Alias the payments_df as \"payments\" for clarity\n",
    "payments_df = payments_df.alias(\"payments\")\n",
    "\n",
    "# Define a window specification to partition by order_id\n",
    "window_spec = Window.partitionBy(\"payments.Order_id\")\n",
    "\n",
    "# Calculate the count of payment methods per order\n",
    "payment_count_per_order = payments_df.withColumn(\"payment_method_count\", count(\"payments.Payment_type\").over(window_spec))\n",
    "payment_count_per_order  = payment_count_per_order .withColumnRenamed(\"Payment_type\", \"new_payment_types\")\n",
    "# Calculate the average and standard deviation of payment installments per order\n",
    "payment_stats_per_order = payments_df.withColumn(\"avg_installments\", avg(\"payments.Payment_installments\").over(window_spec)) \\\n",
    "                                     .withColumn(\"stddev_installments\", stddev(\"payments.Payment_installments\").over(window_spec))\n",
    "payment_stats_per_order = payment_stats_per_order.withColumnRenamed(\"Payment_type\", \"new_payment_type\")\n",
    "# Define thresholds for detecting unusual payment patterns\n",
    "max_payment_methods = 1  # Maximum allowed payment methods per order\n",
    "max_avg_installments = 3  # Maximum allowed average installments per order\n",
    "max_stddev_installments = 2  # Maximum allowed standard deviation of installments per order\n",
    "\n",
    "# Detect potential fraud by filtering orders with unusual payment patterns\n",
    "potential_fraud_orders = payments_df.join(payment_count_per_order, \"Order_id\", \"inner\") \\\n",
    "                                    .join(payment_stats_per_order, \"Order_id\", \"inner\") \\\n",
    "                                    .filter((col(\"payment_method_count\") > max_payment_methods) |\n",
    "                                            (col(\"avg_installments\") > max_avg_installments) |\n",
    "                                            (col(\"stddev_installments\") > max_stddev_installments)) \n",
    "# potential_fraud_orders.select(\"payments.Order_id\", \"payments.Payment_type\".alias(\"payment_type\"),\n",
    "#                                             \"payments.Payment_installments\",\n",
    "#                                             col(\"payment_method_count\"), col(\"avg_installments\"), col(\"stddev_installments\"))\n",
    "\n",
    "selected_columns = potential_fraud_orders.select(\n",
    "    payments_df[\"Order_id\"],\n",
    "    payments_df[col(\"Payment_type\")],\n",
    "    payments_df[col(\"Payment_installments\")],\n",
    "    col(\"payment_method_count\"),\n",
    "    col(\"avg_installments\"),\n",
    "    col(\"stddev_installments\")\n",
    ")\n",
    "\n",
    "# Show potential fraud orders\n",
    "selected_columns.show()\n",
    "# # Show potential fraud orders\n",
    "# potential_fraud_orders.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the product and order items datasets to get product revenue\n",
    "joined_df = products_df.join(order_items_df, \"product_id\", \"inner\")\n",
    "\n",
    "\n",
    "# Join the product and order items datasets to get product revenue\n",
    "joined_df = products_df.join(order_items_df, \"product_id\", \"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the revenue for each product by multiplying price and quantity\n",
    "revenue_df = joined_df.withColumn(\"total_revenue\", col(\"price\") * col(\"order_item_id\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by product category name and calculate the total revenue for each category\n",
    "category_revenue_df = revenue_df.groupBy(\"product_category_name\") \\\n",
    "                                .agg(sum(\"total_revenue\").alias(\"category_total_revenue\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rank the categories by total revenue in descending order\n",
    "window_spec = Window.orderBy(col(\"category_total_revenue\").desc())\n",
    "ranked_category_revenue_df = category_revenue_df.withColumn(\"rank\", F.rank().over(window_spec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/12 07:34:53 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/09/12 07:34:53 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/09/12 07:34:53 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/09/12 07:34:54 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/09/12 07:34:54 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/09/12 07:34:55 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/09/12 07:34:55 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/09/12 07:34:55 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/09/12 07:34:55 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+----------------------+\n",
      "|product_category_name|category_total_revenue|\n",
      "+---------------------+----------------------+\n",
      "|         beleza_saude|    1347468.4900000044|\n",
      "|   relogios_presentes|    1259634.5800000038|\n",
      "|      cama_mesa_banho|    1228795.4600000102|\n",
      "| informatica_acess...|    1135454.6400000013|\n",
      "|        esporte_lazer|     1082435.419999999|\n",
      "+---------------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select the top 5 product categories\n",
    "top_5_categories = ranked_category_revenue_df.filter(col(\"rank\") <= 5).select(\"product_category_name\", \"category_total_revenue\")\n",
    "\n",
    "# Show the result\n",
    "top_5_categories.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 20:==============>                                           (1 + 3) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+----------+------------------+\n",
      "|Payment_type|Order_purchase_month|num_orders|  avg_installments|\n",
      "+------------+--------------------+----------+------------------+\n",
      "| credit_card|             2016-09|         3|               2.0|\n",
      "|      boleto|             2016-10|        63|               1.0|\n",
      "| credit_card|             2016-10|       253| 4.405511811023622|\n",
      "|     voucher|             2016-10|        11|               1.0|\n",
      "|  debit_card|             2016-10|         2|               1.0|\n",
      "| credit_card|             2016-12|         1|               1.0|\n",
      "|      boleto|             2017-01|       197|               1.0|\n",
      "|  debit_card|             2017-01|         9|               1.0|\n",
      "| credit_card|             2017-01|       582| 3.795883361921098|\n",
      "|     voucher|             2017-01|        33|               1.0|\n",
      "|     voucher|             2017-02|        69|               1.0|\n",
      "|  debit_card|             2017-02|        13|               1.0|\n",
      "| credit_card|             2017-02|      1347|3.4306784660766962|\n",
      "|      boleto|             2017-02|       398|               1.0|\n",
      "|  debit_card|             2017-03|        31|               1.0|\n",
      "| credit_card|             2017-03|      2008|3.6557539682539684|\n",
      "|     voucher|             2017-03|       123|               1.0|\n",
      "|      boleto|             2017-03|       590|               1.0|\n",
      "|      boleto|             2017-04|       496|               1.0|\n",
      "|     voucher|             2017-04|       115|               1.0|\n",
      "+------------+--------------------+----------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Load the necessary dataset\n",
    "payments_df = spark.read.csv(\"Data/olist_order_payments_dataset.csv\", header=True, inferSchema=True)\n",
    "orders_df = spark.read.csv(\"Data/olist_orders_dataset.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Convert the timestamp columns to date format\n",
    "\n",
    "orders_df = orders_df.withColumn(\"Order_purchase_month\", F.date_format(col(\"Order_purchase_timestamp\"), \"yyyy-MM\"))\n",
    "\n",
    "# Join the payments and orders datasets on the order_id\n",
    "joined_df = payments_df.join(orders_df, \"Order_id\", \"inner\")\n",
    "\n",
    "# Group by payment type, purchase month, and count the number of orders and average payment installments\n",
    "payment_analysis_df = joined_df.groupBy(\"Payment_type\", \"Order_purchase_month\") \\\n",
    "    .agg(\n",
    "        F.countDistinct(\"Order_id\").alias(\"num_orders\"),\n",
    "        F.avg(\"Payment_installments\").alias(\"avg_installments\")\n",
    "    ) \\\n",
    "    .orderBy(\"Order_purchase_month\")\n",
    "\n",
    "# Show the result\n",
    "payment_analysis_df.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'orders_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/user/Documents/Fusemachines/Spark_FInal_Projetc/ques.ipynb Cell 13\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/user/Documents/Fusemachines/Spark_FInal_Projetc/ques.ipynb#X15sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msql\u001b[39;00m \u001b[39mimport\u001b[39;00m WindowSpec\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/user/Documents/Fusemachines/Spark_FInal_Projetc/ques.ipynb#X15sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# Convert the timestamp columns to date format\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/user/Documents/Fusemachines/Spark_FInal_Projetc/ques.ipynb#X15sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m orders_df \u001b[39m=\u001b[39m orders_df\u001b[39m.\u001b[39mwithColumn(\u001b[39m\"\u001b[39m\u001b[39mOrder_purchase_month\u001b[39m\u001b[39m\"\u001b[39m, F\u001b[39m.\u001b[39mdate_format(col(\u001b[39m\"\u001b[39m\u001b[39mOrder_purchase_timestamp\u001b[39m\u001b[39m\"\u001b[39m), \u001b[39m\"\u001b[39m\u001b[39myyyy-MM\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/Documents/Fusemachines/Spark_FInal_Projetc/ques.ipynb#X15sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# Join the payments and orders datasets on the order_id\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/Documents/Fusemachines/Spark_FInal_Projetc/ques.ipynb#X15sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m joined_df \u001b[39m=\u001b[39m payments_df\u001b[39m.\u001b[39mjoin(orders_df, \u001b[39m\"\u001b[39m\u001b[39mOrder_id\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39minner\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'orders_df' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import WindowSpec\n",
    "\n",
    "\n",
    "\n",
    "# Convert the timestamp columns to date format\n",
    "\n",
    "orders_df = orders_df.withColumn(\"Order_purchase_month\", F.date_format(col(\"Order_purchase_timestamp\"), \"yyyy-MM\"))\n",
    "\n",
    "# Join the payments and orders datasets on the order_id\n",
    "joined_df = payments_df.join(orders_df, \"Order_id\", \"inner\")\n",
    "\n",
    "# Group by payment type, purchase month, and calculate various statistics on payment installments\n",
    "window_spec = Window.partitionBy(\"Payment_type\", \"Order_purchase_month\")\n",
    "\n",
    "payment_analysis_df = joined_df.groupBy(\"Payment_type\", \"Order_purchase_month\") \\\n",
    "    .agg(\n",
    "        F.countDistinct(\"Order_id\").alias(\"num_orders\"),\n",
    "        F.avg(\"Payment_installments\").alias(\"avg_installments\"),\n",
    "        F.stddev(\"Payment_installments\").alias(\"stddev_installments\"),\n",
    "        F.expr(\"percentile_approx(Payment_installments, 0.5)\").alias(\"median_installments\")\n",
    "    ) \\\n",
    "    .orderBy(\"Order_purchase_month\")\n",
    "\n",
    "# Show the result\n",
    "payment_analysis_df.show()\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_1",
   "language": "python",
   "name": "venv_1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
